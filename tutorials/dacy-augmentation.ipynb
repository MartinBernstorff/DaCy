{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to augmenters\n",
    "\n",
    "This notebook provides a short introduction to some of the tools for augmentation included in `DaCy`. For information on how to  conduct robustness test of your models please see `dacy-robustness.ipynb`.\n",
    "\n",
    "Let's start out by seeing how different augmenters change your text. The augmenters included in `DaCy` is based of the spaCy augmenters, which mean you can you them both for training and behavoiral testing. Thus it needs to work on the `Example` class from spaCy (as opposed to the `Doc`), so let's write a little helper function that converts a `Doc` to an `Example` and write some text to test on.\n",
    "\n",
    "The `Example` class consist of two Docs one being the reference (or gold-standard) which contain the labeled data the other being the predicted, which contains the prediction of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dacy\n",
      "  Using cached dacy-1.0.0-py3-none-any.whl (148 kB)\n",
      "Collecting spacy>=3.0.3\n",
      "  Using cached spacy-3.1.0-cp38-cp38-macosx_10_9_x86_64.whl (6.0 MB)\n",
      "Collecting spacy-transformers>=1.0.1\n",
      "  Using cached spacy_transformers-1.0.3-py2.py3-none-any.whl (39 kB)\n",
      "Collecting pandas>=1.0.0\n",
      "  Using cached pandas-1.3.0-cp38-cp38-macosx_10_9_x86_64.whl (11.4 MB)\n",
      "Collecting tqdm>=4.42.1\n",
      "  Using cached tqdm-4.61.2-py2.py3-none-any.whl (76 kB)\n",
      "Collecting pytz>=2017.3\n",
      "  Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting numpy>=1.17.3\n",
      "  Using cached numpy-1.21.0-cp38-cp38-macosx_10_9_x86_64.whl (16.9 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/au561649/.virtualenvs/dacy_tutorials/lib/python3.8/site-packages (from pandas>=1.0.0->dacy) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/au561649/.virtualenvs/dacy_tutorials/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.0.0->dacy) (1.16.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.5-cp38-cp38-macosx_10_9_x86_64.whl (18 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Using cached pathy-0.6.0-py3-none-any.whl (42 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.5-cp38-cp38-macosx_10_9_x86_64.whl (105 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.5-cp38-cp38-macosx_10_9_x86_64.whl (31 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Using cached srsly-2.4.1-cp38-cp38-macosx_10_9_x86_64.whl (450 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-21.0-py3-none-any.whl (40 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.4\n",
      "  Using cached catalogue-2.0.4-py3-none-any.whl (16 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Using cached blis-0.7.4-cp38-cp38-macosx_10_9_x86_64.whl (5.8 MB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Using cached pydantic-1.8.2-cp38-cp38-macosx_10_9_x86_64.whl (2.6 MB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.7\n",
      "  Using cached spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Using cached wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: setuptools in /Users/au561649/.virtualenvs/dacy_tutorials/lib/python3.8/site-packages (from spacy>=3.0.3->dacy) (57.0.0)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Using cached typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.0.1-py3-none-any.whl (133 kB)\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.7\n",
      "  Using cached thinc-8.0.7-cp38-cp38-macosx_10_9_x86_64.whl (597 kB)\n",
      "Collecting pyparsing>=2.0.2\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Using cached smart_open-5.1.0-py3-none-any.whl (57 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "Collecting spacy-alignments<1.0.0,>=0.7.2\n",
      "  Using cached spacy_alignments-0.8.3-cp38-cp38-macosx_10_9_x86_64.whl (302 kB)\n",
      "Collecting torch>=1.5.0\n",
      "  Using cached torch-1.9.0-cp38-none-macosx_10_9_x86_64.whl (127.9 MB)\n",
      "Collecting transformers<4.7.0,>=3.4.0\n",
      "  Using cached transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Using cached huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2021.7.6-cp38-cp38-macosx_10_9_x86_64.whl (285 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp38-cp38-macosx_10_11_x86_64.whl (2.2 MB)\n",
      "Collecting click<7.2.0,>=7.1.1\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.0.1-cp38-cp38-macosx_10_9_x86_64.whl (13 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, numpy, murmurhash, idna, cymem, click, chardet, certifi, catalogue, wasabi, typer, tqdm, srsly, smart-open, requests, regex, pyparsing, pydantic, preshed, MarkupSafe, joblib, filelock, blis, tokenizers, thinc, spacy-legacy, sacremoses, pathy, packaging, jinja2, huggingface-hub, transformers, torch, spacy-alignments, spacy, pytz, spacy-transformers, pandas, dacy\n",
      "Successfully installed MarkupSafe-2.0.1 blis-0.7.4 catalogue-2.0.4 certifi-2021.5.30 chardet-4.0.0 click-7.1.2 cymem-2.0.5 dacy-1.0.0 filelock-3.0.12 huggingface-hub-0.0.8 idna-2.10 jinja2-3.0.1 joblib-1.0.1 murmurhash-1.0.5 numpy-1.21.0 packaging-21.0 pandas-1.3.0 pathy-0.6.0 preshed-3.0.5 pydantic-1.8.2 pyparsing-2.4.7 pytz-2021.1 regex-2021.7.6 requests-2.25.1 sacremoses-0.0.45 smart-open-5.1.0 spacy-3.1.0 spacy-alignments-0.8.3 spacy-legacy-3.0.8 spacy-transformers-1.0.3 srsly-2.4.1 thinc-8.0.7 tokenizers-0.10.3 torch-1.9.0 tqdm-4.61.2 transformers-4.6.1 typer-0.3.2 typing-extensions-3.10.0.0 urllib3-1.26.6 wasabi-0.8.2\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Users/au561649/.virtualenvs/dacy_tutorials/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dacy\n",
    "from spacy.training import Example\n",
    "from typing import List, Callable, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_example(doc):\n",
    "    return Example(doc, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/au561649/.virtualenvs/dacy_tutorials/lib/python3.8/site-packages/spacy/util.py:730: UserWarning: [W095] Model 'da_dacy_-l-ctra_small_tft' (0.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = dacy.load(\"da_dacy_small_tft-0.0.0\")\n",
    "doc = nlp(\"Peter Schmeichel mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\")\n",
    "example = doc_to_example(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how some of the simple augmenters transform the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training.augment import create_lower_casing_augmenter\n",
    "from dacy.augmenters import (create_keyboard_augmenter, create_pers_augmenter,\n",
    "                             create_spacing_augmenter)\n",
    "from dacy.datasets import danish_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_aug = create_lower_casing_augmenter(level=1)\n",
    "keyboard_05 = create_keyboard_augmenter(doc_level=1, char_level=0.05, keyboard = \"QWERTY_DA\")\n",
    "keyboard_15 = create_keyboard_augmenter(doc_level=1, char_level=0.15, keyboard = \"QWERTY_DA\")\n",
    "space_aug = create_spacing_augmenter(doc_level=1, spacing_level=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lower_aug` will change all text to lowercase as the level is set to 1 (100%), `keyboard_05` and `keyboard_15` will change 5% or 15% of all characters to a character on a neighbouring key on a Danish QWERTY keyboard (replace `DA` with `EN` for English), and `space_aug` will remove 40% of all whitespaces. The augmenters takes in an `Example` and modify both the reference and the predicted `Doc`s in the Example and makes sure that spans for NER, POS etc. remain correct. Let's see how the text looks. As the augmenters can return multiple examples we utilize the `next` to extract the first (and only) example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peter schmeichel mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod england.\n",
      "Peter Schmekchel mener også, at det dansje landshole anno 2021 tilh-rer verdenstoppen og kan vinde den kommende kamp mod England.\n",
      "Perer Schm3icheo mener også, ag det dansme lanfshold anno 2021 tilhørwr verdenat9ppen og jan cinde den kommende kamp mod Emglanf.\n",
      "Peter Schmeichelmener også, atdetdanskelandsholdanno 2021tilhører verdenstoppen og kanvindeden kommende kamp modEngland.\n"
     ]
    }
   ],
   "source": [
    "for aug in [lower_aug, keyboard_05, keyboard_15, space_aug]:\n",
    "    aug_example = next(aug(nlp, example))       # augment the example\n",
    "    doc = aug_example.y                         # extract the reference doc\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty neat, right? \n",
    "`DaCy` also includes a more sophisticated augmenter for augmenting names. `create_pers_augmenter` which is highly flexible, and can augment names to fit a certain pattern (e.g. first_name, last_name; abbreviated_first_name, last_name) or replace names with one sampled from a dictionary. `DaCy` provides four utility functions for constructing such name dictionaries: `danish_names`, `female_names`, `male_names`, and `muslim_names` (see the README in `datasets/lookup_tables` for sources). The dictionaries are composed of the keys `first_name` and `last_name` which each contain a list of names to sample from. The `pers_augmenter` uses this dictionary when it replaces names to respect first and last names. Let's go through a couple of examples to demonstrate how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['first_name', 'last_name'])\n",
      "['Marie', 'Anna', 'Margrethe', 'Karen', 'Kirstine']\n",
      "['Jensen', 'Nielsen', 'Hansen', 'Pedersen', 'Andersen']\n"
     ]
    }
   ],
   "source": [
    "print(danish_names().keys())\n",
    "print(danish_names()[\"first_name\"][0:5])\n",
    "print(danish_names()[\"last_name\"][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_texts(texts: List[str], augmenter: Callable) -> Iterator[Example]:\n",
    "    \"\"\"Takes a list of strings and yields augmented examples\"\"\"\n",
    "    docs = nlp.pipe(texts)\n",
    "    for doc in docs:\n",
    "        ex = Example(doc, doc)\n",
    "        aug = augmenter(nlp, ex)\n",
    "        yield next(aug).y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ricard Melgaard var en dansk digter og forfatter\n",
      "1, 2, 3, Nikolaj Bengtsson er en mur\n",
      "May Kjellerup mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Hans Christian Andersen var en dansk digter og forfatter\",\n",
    "    \"1, 2, 3, Schmeichel er en mur\",\n",
    "    \"Peter Schmeichel mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\"\n",
    "    ]\n",
    "\n",
    "# Create a dictionary to use for name replacement\n",
    "dk_name_dict = danish_names()\n",
    "\n",
    "\n",
    "# force_pattern augments PER entities to fit the format and length of `patterns`. Patterns allows you to specificy arbitrary\n",
    "# combinations of \"fn\" (first names), \"ln\" (last names), \"abb\" (abbreviated to first character) and \"abbpunct\" (abbreviated\n",
    "# to first character + \".\") separeated by \",\". If keep_name=True, the augmenter will not change names, but if force_pattern_size\n",
    "# is True it will make them fit the length and potentially abbreviate names. \n",
    "pers_aug = create_pers_augmenter(dk_name_dict, force_pattern_size=True, keep_name=False, patterns=[\"fn,ln\"])\n",
    "augmented_docs = augment_texts(texts, pers_aug)\n",
    "for d in augmented_docs:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H. Christian Andersen var en dansk digter og forfatter\n",
      "1, 2, 3, S. er en mur\n",
      "P. Schmeichel mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\n"
     ]
    }
   ],
   "source": [
    "# Here's an example with keep_name=True and force_pattern_size=False which simply abbreviates first names\n",
    "abb_aug = create_pers_augmenter(dk_name_dict, force_pattern_size=False, keep_name=True, patterns=[\"abbpunct\"])\n",
    "augmented_docs = augment_texts(texts, abb_aug)\n",
    "for d in augmented_docs:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. Wollesen var en dansk digter og forfatter\n",
      "1, 2, 3, Richardt Korsholm er en mur\n",
      "L. Tobiasen mener også, at det danske landshold anno 2021 tilhører verdenstoppen og kan vinde den kommende kamp mod England.\n"
     ]
    }
   ],
   "source": [
    "# patterns can also take a list of patterns to replace from (which can be weighted using the\n",
    "# patterns_prob argument. The pattern to use is sampled for each entity. \n",
    "# This setting is especially useful for finetuning models.\n",
    "multiple_pats = create_pers_augmenter(dk_name_dict, \n",
    "                                      force_pattern_size=True,\n",
    "                                      keep_name=False,\n",
    "                                      patterns=[\"fn,ln\", \"abbpunct,ln\", \"fn,ln,ln,ln\"])\n",
    "augmented_docs = augment_texts(texts, multiple_pats)\n",
    "for d in augmented_docs:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with the options for `create_pers_augmenter` to get a feeling for how it works and check out the docs.\n",
    "\n",
    "The main strength of making the augmenters work with SpaCy is that we ensure that the spans of the augmented data still has the correct tags even though we add or remove words. This allows us to use them with gold-standard tagged datasets such as DaNE and use them for both training and evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Hans Christian Andersen, dansk) \t\t (Ib Bojesen Witt Walther, dansk)\n",
      "(Schmeichel,) \t\t (Bernhard Østergård,)\n",
      "(Peter Schmeichel, danske, England) \t\t (B. Knudsen, danske, England)\n"
     ]
    }
   ],
   "source": [
    "docs = nlp.pipe(texts)\n",
    "augmented_docs = augment_texts(texts, multiple_pats)\n",
    "\n",
    "# Check that the added/removed PER entities are still tagged as entities\n",
    "for doc, aug_doc in zip(docs, augmented_docs):\n",
    "    print(doc.ents, \"\\t\\t\", aug_doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributing\n",
    "\n",
    "We highly encourage others to contribute more augmenters that cover a wider range of use cases. For inspiration on how to make your own, checkout the source code for the ones included in `DaCy` in the `dacy/augmenters` folder and SpaCy's documentation [here](https://spacy.io/usage/training#data-augmentation). If you have a good idea for one or encounter any problems, please open an issue or write on the discussion board."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69f6eb96a37a43867553731f8edb0a55a5852b5c9c304e04d7bd7872e5b1c11d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('dacy': venv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}